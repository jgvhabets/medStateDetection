{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import itertools\n",
    "import pdb \n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "from os.path import join\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "%matplotlib inline\n",
    "\n",
    "from mne.filter import filter_data\n",
    "\n",
    "import scipy\n",
    "from scipy.signal import welch, decimate, periodogram, find_peaks\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, mannwhitneyu, spearmanr\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from statistics import mode\n",
    "import math  \n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting bradykinesia features from downloaded Accelerometer data\n",
    "\n",
    "The function requires already downloaded (Apple watch) accelerometry (acc) data, saved in csv-files, per day, per patient. Saved with filenames as in Notebook Data Download (e.g. 'RCS02_10Jun2020_userAcc.csv')\n",
    "The function requires all acc-data files to be in one folder, with the patient-code as a name (e.g. RCS02).\n",
    "\n",
    "The function will extract features per day, write csv files with all features per day, for every day in the defined time span in the function input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/roee/Starr_Lab_Folder/Data_Analysis/medStateDetection/results\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.path.dirname(os.getcwd())) # changed to main folder, instead of results\n",
    "print(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Accelerometry data, bandpass Filter, and Extract Features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filterWatchData(pt, y0,m0,d0,y1,m1,d1):\n",
    "    \n",
    "    '''\n",
    "    Input: \n",
    "    - pt: patient as string (e.g. 'RCS02')\n",
    "    - y0,m0,d0 : start date of desired timeperiod (year, month, date, e.g. 2020, 5, 1)\n",
    "    - y1,m1,d1 : end date of desired timeperiod (year, month, date, e.g. 2020, 6, 1)\n",
    "    \n",
    "    Calculates features per day.\n",
    "    Saves features as .csv\n",
    "    \n",
    "    Returns: one DF with raw AW data, and 1 DF with filtered AW data\n",
    "    '''\n",
    "\n",
    "    sr = 50 # sample ratio apple watch accelerometry, used in filter function below\n",
    "    \n",
    "    bandPassLow = 0 # lower cutoff of bandpass filter\n",
    "    bandPassHigh = 3.5 # higher cutoff of bandpass filter\n",
    "    \n",
    "    filteredData = {} # empty dict to store filtered rcs data\n",
    "    \n",
    "    # define days in given timespan\n",
    "    def datetime_range(start=None, end=None):\n",
    "        span = end - start\n",
    "        for i in range(span.days + 1):\n",
    "            yield start + timedelta(days=i)\n",
    "    # create list with datetime dates for every day in timespan\n",
    "    datetimeDays = list(datetime_range(start=datetime(y0, m0, d0), end=datetime(y1, m1, d1)))\n",
    "    \n",
    "    # extract all file 'userAccel.csv'-filenames from specified patient-folder\n",
    "    patient_dir_name = os.path.join(path,'data',pt)\n",
    "    folderFiles= [s for s in listdir(patient_dir_name) if s[-15:] =='watch_accel.csv'] \n",
    "        \n",
    "    for fileDay in datetimeDays: # loop over all days in requested timespan\n",
    "        # define name of day-file\n",
    "        day = fileDay.strftime(\"%d\") # generate 2-digit day code\n",
    "        month = fileDay.strftime(\"%d\") # generate 2-digit month code\n",
    "        year = fileDay.strftime(\"%Y\") # generate 4-digit year code\n",
    "        fileName = '%s_%s%s%s_watch_accel.csv' % (pt,year,month,day) # first pt is for specific pt-folder\n",
    "        # check if acc-data file exist in folder, if not: skip day and continue with next\n",
    "        if fileName in folderFiles:\n",
    "            fileName = fileName # go on\n",
    "        else:\n",
    "            print('no file for %s' %fileName)\n",
    "            continue # skips rest of itiration and takes next iteration\n",
    "            \n",
    "        # read csv file\n",
    "        csv_full_path = os.path.join(path,pt,fileName)\n",
    "        rawFile = pd.read_csv(csv_full_path , header=0) \n",
    "        \n",
    "        ## DATA LOADING\n",
    "        timeStamps = [] # create empty list for timestamps\n",
    "        timeDelta = [0] # list for time difference per sample vs previous sample (0 for fist) (check for timestamp consistency)\n",
    "        for row in np.arange(len(rawFile['time'])): # loop over every sample\n",
    "            timeStamps.append(datetime.fromtimestamp(rawFile['time'][row])) # add timestamp to list\n",
    "            if row > 0: # add timediff to a list, except for first sample...\n",
    "                timeDelta.append((timeStamps[row] - timeStamps[row-1]).total_seconds())\n",
    "        # select only acc axes\n",
    "        dat = rawFile[['x','y','z']].rename(columns={\"x\": \"X\", \"y\": \"Y\", \"z\": \"Z\"})\n",
    "        # calculate raw SVM before filtering\n",
    "        dat['SVM'] = np.sqrt(dat['X']**2 + dat['Y']**2 + dat['Z']**2 )\n",
    "        dat.insert(loc=0, column='timeStamp', value=timeStamps) # add timestamps as first column\n",
    "        # dat is now ready raw acc file\n",
    "        \n",
    "        ## DATA FILTERING\n",
    "        dat = dat.sort_values(by=['timeStamp']).reset_index(drop=True) # sort by timestamp and reset indices\n",
    "        # filter raw RCS acc with wrist-feature relevant bandwidths \n",
    "        # make new dataframe for filtered data, with same timestamps\n",
    "        filtered = pd.DataFrame(data = dat['timeStamp'], columns = ['timeStamp'])\n",
    "        \n",
    "        for col in ['X' ,'Y', 'Z', 'SVM']: # loop over all acc-data columns to filter\n",
    "            # bandpass filter excluding tremor frequencies > 4hz\n",
    "            filteredCol = filter_data(np.array(dat[col]),sr,bandPassLow,bandPassHigh,method='iir',verbose='WARNING')\n",
    "            # filtered data per column stored in dat, write dat to dataframe column in filtered data DF\n",
    "            filtered[col] = filteredCol\n",
    "        \n",
    "        filteredData[day+month] = filtered\n",
    "        \n",
    "    \n",
    "    return filteredData\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bradykinesia faetures extracted from Github Mahadevan 2020\n",
    "## Source: https://github.com/NikhilMahadevan/analyze-tremor-bradykinesia-PD\n",
    "\n",
    "def histogram(signal_x):\n",
    "    '''\n",
    "    Calculate histogram of sensor signal.\n",
    "    :param signal_x: 1-D numpy array of sensor signal\n",
    "    :return: Histogram bin values, descriptor\n",
    "    '''\n",
    "    descriptor = np.zeros(3)\n",
    "\n",
    "    ncell = np.ceil(np.sqrt(len(signal_x)))\n",
    "\n",
    "    max_val = np.nanmax(signal_x.values)\n",
    "    min_val = np.nanmin(signal_x.values)\n",
    "\n",
    "    delta = (max_val - min_val) / (len(signal_x) - 1)\n",
    "\n",
    "    descriptor[0] = min_val - delta / 2\n",
    "    descriptor[1] = max_val + delta / 2\n",
    "    descriptor[2] = ncell\n",
    "\n",
    "    h = np.histogram(signal_x, ncell.astype(int), range=(min_val, max_val))\n",
    "\n",
    "    return h[0], descriptor\n",
    "\n",
    "def dominant_frequency(signal_df, sampling_rate, cutoff ):\n",
    "    '''\n",
    "    Calculate dominant frequency of sensor signals.\n",
    "    :param signal_df: Pandas DataFrame housing desired sensor signals\n",
    "    :param sampling_rate: sampling rate of sensor signal\n",
    "    :param cutoff: desired cutoff for filter\n",
    "    :param channels: channels of signal to measure dominant frequency\n",
    "    :return: Pandas DataFrame of calculated dominant frequency for each signal channel\n",
    "    '''\n",
    "    dominant_freq_df = pd.DataFrame()\n",
    "\n",
    "    signal_x = signal_df\n",
    "\n",
    "    padfactor = 1\n",
    "    dim = signal_x.shape\n",
    "    nfft = 2 ** ((dim[0] * padfactor).bit_length())\n",
    "\n",
    "    freq_hat = np.fft.fftfreq(nfft) * sampling_rate\n",
    "    freq = freq_hat[0: int(nfft / 2)]\n",
    "\n",
    "    idx1 = freq <= cutoff\n",
    "    idx_cutoff = np.argwhere(idx1)\n",
    "    freq = freq[idx_cutoff]\n",
    "\n",
    "    sp_hat = np.fft.fft(signal_x, nfft)\n",
    "    sp = sp_hat[0: int(nfft / 2)] * np.conjugate(sp_hat[0: int(nfft / 2)])\n",
    "    sp = sp[idx_cutoff]\n",
    "    sp_norm = sp / sum(sp)\n",
    "\n",
    "    max_freq = freq[sp_norm.argmax()][0]\n",
    "    max_freq_val = sp_norm.max().real\n",
    "\n",
    "    idx2 = (freq > max_freq - 0.5) * (freq < max_freq + 0.5)\n",
    "    idx_freq_range = np.where(idx2)[0]\n",
    "    dom_freq_ratio = sp_norm[idx_freq_range].real.sum()\n",
    "\n",
    "    # Calculate spectral flatness\n",
    "    spectral_flatness = 10.0*np.log10(stats.mstats.gmean(sp_norm)/np.mean(sp_norm))\n",
    "\n",
    "    # Estimate spectral entropy\n",
    "    spectral_entropy_estimate = 0\n",
    "    for isess in range(len(sp_norm)):\n",
    "        if sp_norm[isess] != 0:\n",
    "            logps = np.log2(sp_norm[isess])\n",
    "        else:\n",
    "            logps = 0\n",
    "        spectral_entropy_estimate = spectral_entropy_estimate - logps * sp_norm[isess]\n",
    "\n",
    "    spectral_entropy_estimate = spectral_entropy_estimate / np.log2(len(sp_norm))\n",
    "    # spectral_entropy_estimate = (spectral_entropy_estimate - 0.5) / (1.5 - spectral_entropy_estimate)\n",
    "\n",
    "    dominant_freq_df['_dom_freq_value'] = [max_freq]\n",
    "    dominant_freq_df['_dom_freq_magnitude'] = [max_freq_val]\n",
    "    dominant_freq_df['_dom_freq_ratio'] = [dom_freq_ratio]\n",
    "    dominant_freq_df['_spectral_flatness'] = [spectral_flatness[0].real]\n",
    "    dominant_freq_df['_spectral_entropy'] = [spectral_entropy_estimate[0].real]\n",
    "\n",
    "    return dominant_freq_df\n",
    "\n",
    "def signal_entropy(windowData):\n",
    "    data_norm = windowData/np.std(windowData)\n",
    "    h, d = histogram(data_norm)\n",
    "    lowerbound = d[0]\n",
    "    upperbound = d[1]\n",
    "    ncell = int(d[2])\n",
    "\n",
    "    estimate = 0\n",
    "    sigma = 0\n",
    "    count = 0\n",
    "\n",
    "    for n in range(ncell):\n",
    "        if h[n] != 0:\n",
    "            logf = np.log(h[n])\n",
    "        else:\n",
    "            logf = 0\n",
    "        count = count + h[n]\n",
    "        estimate = estimate - h[n] * logf\n",
    "        sigma = sigma + h[n] * logf ** 2\n",
    "\n",
    "    nbias = -(float(ncell) - 1) / (2 * count)\n",
    "\n",
    "    estimate = estimate / count\n",
    "    estimate = estimate + np.log(count) + np.log((upperbound - lowerbound) / ncell) - nbias\n",
    "    \n",
    "    # Scale the entropy estimate to stretch the range\n",
    "    estimate = np.exp(estimate ** 2) - np.exp(0) - 1\n",
    "\n",
    "\n",
    "    \n",
    "    return estimate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractFeatures(pt, filteredData, windowLen=60, sr=50):\n",
    "    '''\n",
    "    Input: \n",
    "    - filteredData = dictionary of filtered acc data, for every day a seperate dataframe\n",
    "    filteredData is automatically result of first function.\n",
    "    - windowLen = desired window length of features in seconds\n",
    "    - sr = sample frequency of recorded accelerometry data, in Hz, AppleWatch accelerometry = 50 Hz.\n",
    "    \n",
    "    Writes feature dataframes per day to .csv\n",
    "    \n",
    "    Returns: One dictionary with feature dataframes.\n",
    "    '''\n",
    "\n",
    "    tDelta = sr*windowLen # time-delta is factor between filtered data sample rate and desired windowlength\n",
    "    \n",
    "    # Define all names of feature-labels which will be calculated over all axes\n",
    "    totalFeatLabels = [] # one list for all feature names (SVM,X,Y and Z)\n",
    "    \n",
    "    for axis in ['SVM','X', 'Y', 'Z']:  # loop over all axes, calculate features per axis    \n",
    "        # add list with features for every axis\n",
    "        featureList = ['_maxAcc','_iqrAcc', '_90prcAcc','_medianAcc','_meanAcc',\n",
    "                       '_stddev','_variance','_coefVar','_accRange',\n",
    "                        '_lowPeaks','_highPeaks', '_time1gAcc','_accEntropy','_jerkRatio',\n",
    "                       '_RMS', \n",
    "                       '_specPow_totalu4Hz', '_specPow_low','_specPow_mid', '_specPow_high',\n",
    "         '_domFreq_magnitude', '_domFreq_ratio', \n",
    "                       '_spectral_flatness', '_spectral_entropy',] #'_domFreq_value', (left out, no variation)\n",
    "        \n",
    "        for feat in featureList:\n",
    "            totalFeatLabels.append(axis+feat)\n",
    "        # features only for XYZ\n",
    "        if np.logical_or(axis == 'X', axis == 'Y'): # ratio RMS only relevant for x y and z\n",
    "            totalFeatLabels.append(axis+'_ratioRMS')\n",
    "        elif axis == 'Z':\n",
    "            totalFeatLabels.append(axis+'_ratioRMS')    \n",
    "\n",
    "        # features only once calculated, in SVM\n",
    "        if axis == 'SVM':\n",
    "            for l in ['_spectralVar','_spectralSmoothness1','_spectrallowPeaks',\n",
    "                     '_spectralSmoothness2','_spectralhighPeaks']:\n",
    "                totalFeatLabels.append(axis+l)\n",
    "            totalFeatLabels.extend(['crossCor_XY','crossCor_XZ','crossCor_YZ'])\n",
    "    \n",
    "    ''' TotalFeatLabels is now a list with all feature labels of X,Y,Z,SVM.\n",
    "    One dataframe per session will be calculated and afterwards merged into one total feature-dataframe.\n",
    "    '''\n",
    "    features = {} # empty dict to store feature-dataframes per day\n",
    "    list_days = filteredData.keys() # define days to calculate features for\n",
    "    for day in list_days: # loop over every day in filteredData\n",
    "        # basis for new feature dataframe is timestamps of filteredData\n",
    "        # Add timestamp of beginning of faeture-window to list for feature dataframe timestamps\n",
    "        timeStamps = filteredData[day]['timeStamp'][::tDelta] # take every timestamp at beginning of a feature window\n",
    "        features[day] = pd.DataFrame(data=timeStamps, columns=['timeStamp'])\n",
    "        \n",
    "        # create dict with empty lists for all feature-names \n",
    "        totalFeatureLists = {} # empty dict to store features in lists for this session\n",
    "        for label in totalFeatLabels:\n",
    "            totalFeatureLists[label] = []\n",
    "        \n",
    "        # CALCULATION OF FEATURES, PER AXIS\n",
    "        for axis in ['SVM','X', 'Y', 'Z']:  # loop over all axes, calculate features per axis\n",
    "            \n",
    "            for windowStart in np.arange(0,len(filteredData[day][axis]),tDelta): # iterate over windows of 120 hz * 60 s       \n",
    "                windowData = filteredData[day][axis][windowStart : windowStart+tDelta] # create windowdata per column and per window\n",
    "                \n",
    "                ## DISTRIBUTIVE AND DESCRIPTIVE FEATURES FROM TIME DOMAIN\n",
    "                                \n",
    "                # max acceleration (source: Griffiths 2012)\n",
    "                maxAcc = np.max(np.abs(windowData)) \n",
    "                totalFeatureLists[axis+'_maxAcc'].append(maxAcc)\n",
    "                \n",
    "                # IQR of acc\n",
    "                iqrAcc = scipy.stats.iqr(windowData)\n",
    "                totalFeatureLists[axis+'_iqrAcc'].append(iqrAcc)\n",
    "                \n",
    "                # 90-th percentile acc, (Rispens 2015 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4296095/?report=classic )\n",
    "                perc90 = np.percentile(np.abs(windowData), 90)\n",
    "                totalFeatureLists[axis+'_90prcAcc'].append(perc90)\n",
    "                \n",
    "                # median of acc\n",
    "                medianAcc = np.median(np.abs(windowData))\n",
    "                totalFeatureLists[axis+'_medianAcc'].append(medianAcc)\n",
    "                \n",
    "                # mean of acc\n",
    "                meanAcc = np.mean(np.abs(windowData))\n",
    "                totalFeatureLists[axis+'_meanAcc'].append(meanAcc)\n",
    "                \n",
    "                # standard deviation\n",
    "                stddev = np.std(np.abs(windowData))\n",
    "                totalFeatureLists[axis+'_stddev'].append(stddev)\n",
    "                \n",
    "                # variance (var = mean(abs(x - x.mean())**2))\n",
    "                var = np.var(np.abs(windowData))\n",
    "                totalFeatureLists[axis+'_variance'].append(var)\n",
    "                \n",
    "                # Coefficient of variance (stddev / mean)\n",
    "                coefVar = scipy.stats.variation(np.abs(windowData))\n",
    "                totalFeatureLists[axis+'_coefVar'].append(coefVar)\n",
    "                \n",
    "                # range in signal value; from Mahadevan-Github \n",
    "                accRange = windowData.max(skipna=True) - windowData.min(skipna=True)\n",
    "                totalFeatureLists[axis+'_accRange'].append( accRange )\n",
    "                \n",
    "                # number of acceleration peaks per axes\n",
    "                # low threshold peaks: activity indication\n",
    "                lowPeaks = len(find_peaks(np.abs(windowData),height=1, threshold=None, distance=600)[0]) # height = required value to be a peak, distance is horizontal distance to allow next peak\n",
    "                totalFeatureLists[axis+'_lowPeaks'].append( lowPeaks )\n",
    "                # high threshold peaks: amount of faster activity\n",
    "                highPeaks = len(find_peaks(np.abs(windowData),height=3, threshold=None, distance=600)[0]) # height = required value to be a peak, distance is horizontal distance to allow next peak\n",
    "                totalFeatureLists[axis+'_highPeaks'].append( highPeaks )\n",
    "                \n",
    "                # % time spent in above 1g acceleration\n",
    "                time1gAcc = np.sum(np.abs(windowData) > 1)/len(windowData)\n",
    "                totalFeatureLists[axis+'_time1gAcc'].append( time1gAcc )\n",
    "\n",
    "                # entropy in accelerometry (source: Mahadevan-github)\n",
    "                sigEntropy = signal_entropy(windowData)\n",
    "                totalFeatureLists[axis+'_accEntropy'].append(sigEntropy)\n",
    "                    \n",
    "                ## jerk ratio/smoothness; rate of acc-changes (Hogan 2009) acc to Mahadevan, PM aimed for 3-sec windows\n",
    "                ampl = np.max(np.abs(windowData))\n",
    "                jerk = windowData.diff(1) * sr #(divided by 1 / sr => multiply with sr)\n",
    "                jerkSqSum = np.sum(jerk ** 2)\n",
    "                scale = 360 * ampl ** 2 / tDelta / sr\n",
    "                meanSqJerk = jerkSqSum / sr / (tDelta / sr * 2)\n",
    "                jerkRatio = meanSqJerk / scale\n",
    "                totalFeatureLists[axis+'_jerkRatio'].append(jerkRatio)                               \n",
    "                \n",
    "                # RMS according to classical definition\n",
    "                meanSqAcc = np.mean(np.square(windowData))\n",
    "                rmsAcc = np.sqrt(meanSqAcc)\n",
    "                totalFeatureLists[axis+'_RMS'].append(rmsAcc )\n",
    "                \n",
    "                # RMS ratio (RMS-axis / RMS-svm) (Sekine '13: https://www.ncbi.nlm.nih.gov/pubmed/24370075)\n",
    "                # rms ratio in mediolateral direction is correlated with walking speed\n",
    "                if  np.logical_or(axis == 'X' , axis == 'Y'):\n",
    "                    svmRMS = np.sqrt(np.mean(np.square(filteredData[day]['SVM'][windowStart : windowStart+tDelta])))\n",
    "                    ratioRMS = rmsAcc / svmRMS\n",
    "                    totalFeatureLists[axis+'_ratioRMS'].append(ratioRMS)\n",
    "                elif axis ==  'Z':\n",
    "                    svmRMS = np.sqrt(np.mean(np.square(filteredData[day]['SVM'][windowStart : windowStart+tDelta])))\n",
    "                    ratioRMS = rmsAcc / svmRMS\n",
    "                    totalFeatureLists[axis+'_ratioRMS'].append(ratioRMS)  \n",
    "\n",
    "\n",
    "                ## FEATURES FROM SPECTRAL DOMAIN                \n",
    "               \n",
    "                # Griffiths: MSP: not described: mean over whole 0.2-4.0? mean per which bin-width?? svm or axis?\n",
    "                freq = np.fft.rfftfreq(len(windowData), d = 1/sr) # define freq's for rfft, resolution (bins/hz) is dependent on windowlength of data\n",
    "                lowFreq = np.logical_and(freq < 3.5, freq > 0.0) # select freq's of interest, total = 0-30hz since sr=60\n",
    "                rfft = np.fft.rfft(windowData) # real fast fourier transform (same as fft[freq > 0])\n",
    "                psd = np.log(np.abs(rfft)**2) # log to normalize (rfft gives same barplot as periodogram and fft)\n",
    "                ## ??? is PSD correct as squared value of magnitude, log for normalization?\n",
    "                psdLow = np.sum(psd[lowFreq]) # sum of psd's between selected freq's\n",
    "                totalFeatureLists[axis+'_specPow_totalu4Hz'].append(psdLow)\n",
    "\n",
    "                ## Evers (preprint 2020) gait cadence in 0.7 - 1.4 hz and 1.4 - 2.8 hz\n",
    "                freqGaitA = np.logical_and(freq < 1.4, freq > 0.7) # select freq's of interest, total = 0-30hz since sr=60\n",
    "                psdGaitA = np.sum(psd[freqGaitA]) # sum of psd's between selected freq's\n",
    "                totalFeatureLists[axis+'_specPow_low'].append(psdGaitA)\n",
    "\n",
    "                freqGaitB = np.logical_and(freq < 2.8, freq > 1.4) # select freq's of interest, total = 0-30hz since sr=60\n",
    "                psdGaitB = np.sum(psd[freqGaitB]) # sum of psd's between selected freq's\n",
    "                totalFeatureLists[axis+'_specPow_mid'].append(psdGaitB)\n",
    "                \n",
    "                freqGaitC = np.logical_and(freq < 3.5, freq > 2.8) # select freq's of interest, total = 0-30hz since sr=60\n",
    "                psdGaitC = np.sum(psd[freqGaitC]) # sum of psd's between selected freq's\n",
    "                totalFeatureLists[axis+'_specPow_high'].append(psdGaitC)          \n",
    "            \n",
    "                # dom freq + ratio + spectral flatness and entropy (source: Mahadevan-github)\n",
    "                domFreqValues = dominant_frequency(windowData, sr, 3) # 4 (3) = cutoff for spectrum too analyze\n",
    "#                 totalFeatureLists[axis+'_domFreq_value'].append( float(domFreqValues['_dom_freq_value']))\n",
    "                totalFeatureLists[axis+'_domFreq_magnitude'].append( float(domFreqValues['_dom_freq_magnitude']))\n",
    "                totalFeatureLists[axis+'_domFreq_ratio'].append(float( domFreqValues['_dom_freq_ratio']))\n",
    "                totalFeatureLists[axis+'_spectral_flatness'].append( float(domFreqValues['_spectral_flatness']))\n",
    "                totalFeatureLists[axis+'_spectral_entropy'].append( float(domFreqValues['_spectral_entropy']))\n",
    "                \n",
    "                 \n",
    "        \n",
    "                \n",
    "                if axis == 'SVM':\n",
    "                    # auto-correlation between x-y-z axes\n",
    "                    crossCorXY = pearsonr(windowData, filteredData[day]['Y'][windowStart : windowStart+tDelta])\n",
    "                    crossCorXZ = pearsonr(windowData, filteredData[day]['Z'][windowStart : windowStart+tDelta])\n",
    "                    crossCorYZ = pearsonr(filteredData[day]['Y'][windowStart : windowStart+tDelta], filteredData[day]['Z'][windowStart : windowStart+tDelta])\n",
    "                    totalFeatureLists['crossCor_XY'].append(crossCorXY[0])\n",
    "                    totalFeatureLists['crossCor_XZ'].append(crossCorXZ[0])\n",
    "                    totalFeatureLists['crossCor_YZ'].append(crossCorYZ[0])\n",
    "                    \n",
    "                    # spectral variability and approximation of smoothness and PSD-line-length (importance ref by Beck 2019, Balasubramanian 20120)\n",
    "                    normPSD = psd/psd[0] # normalize PSD by first value DC-normalization (Subramaninian '12')\n",
    "                    spectralVar = np.var(normPSD[lowFreq])\n",
    "                    totalFeatureLists[axis+'_spectralVar'].append(spectralVar)\n",
    "                    # approximation of spectral length; find_peaks finds all small peaks, \n",
    "                    # thresholds represent the distance from the peak to the neighbouring points\n",
    "                    # sum of threshold-values indicates the distances the PSD-line makes to the peaks\n",
    "                    ind, treshs = find_peaks(normPSD[lowFreq],height=None, threshold=0.05, distance=1) # first value returns peak-indices, second tresholds                  \n",
    "                    spectralSmoothness = np.sum(treshs['left_thresholds']+treshs['right_thresholds'])\n",
    "                    totalFeatureLists[axis+'_spectralSmoothness1'].append(spectralSmoothness)\n",
    "                    # number of peaks found\n",
    "                    spectralPeaks = len(ind)\n",
    "                    totalFeatureLists[axis+'_spectrallowPeaks'].append(spectralPeaks)\n",
    "                    \n",
    "                    # same smoothness and peaks, with higher peak-threshold\n",
    "                    ind, treshs = find_peaks(normPSD[lowFreq],height=None, threshold=0.1, distance=2) # first value returns peak-indices, second tresholds                  \n",
    "                    spectralSmoothness = np.sum(treshs['left_thresholds']+treshs['right_thresholds'])\n",
    "                    totalFeatureLists[axis+'_spectralSmoothness2'].append(spectralSmoothness)\n",
    "                    # number of peaks found\n",
    "                    spectralPeaks = len(ind)\n",
    "                    totalFeatureLists[axis+'_spectralhighPeaks'].append(spectralPeaks)\n",
    "\n",
    "        ''' All features are calculated over all axes; \n",
    "        now writing all lists with features in to a feature dataframe per session in featureDict'''\n",
    "        \n",
    "        # fill every column with calculated feature values\n",
    "        for col in totalFeatLabels:\n",
    "            features[day][col] = totalFeatureLists[col]\n",
    "    \n",
    "        # save features per patient\n",
    "        fileName = '%s_%s%s%s_%isec_features.csv' % (pt, year,month,day, windowLen)\n",
    "        csv_full_file_write = os.path.join(path,'results',pt,fileName)\n",
    "        features[day].to_csv(csv_full_file_write, index=False)\n",
    "    \n",
    "    return features\n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file for RCS02_08Jun2020_userAccel.csv\n",
      "no file for RCS02_09Jun2020_userAccel.csv\n",
      "doch\n",
      "doch\n"
     ]
    }
   ],
   "source": [
    "# execute data filtering and feature extraction\n",
    "\n",
    "filteredData = load_filterWatchData('RCS02', 2020,6,8, 2020,6,11)\n",
    "features = extractFeatures(pt='RCS02', filteredData=filteredData, )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
